{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbBCb7spNK7G"
   },
   "source": [
    "# Basic Machine Learning in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIaY6p2fsoWO"
   },
   "source": [
    "## Task: Sentiment Analysis\n",
    "\n",
    "* Sentiment Analysis is a task to determine the emotional tone behind a body of text.\n",
    "* This is especially useful in understanding customer opinions in product reviews, where sentiments are broadly categorized as positive or negative.\n",
    "* For example, analyzing reviews on an e-commerce site can reveal customer satisfaction levels and preferences, aiding in better decision-making for both businesses and potential buyers.\n",
    "* Similarly, in the context of book reviews, sentiment analysis can provide insights into readers' reception of the content, style, and storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceC9-9zRuH7L"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before diving into the models, the first step is to acquire and understand the dataset we will be using.\n",
    "\n",
    "1. **Download Dataset**\n",
    "* Navigate to the [Large Movie Review Dataset v1.0 page](https://ai.stanford.edu/~amaas/data/sentiment/) provided by Stanford AI Lab.\n",
    "\n",
    "* Download the dataset titled 'Large Movie Review Dataset v1.0'.\n",
    "\n",
    "* This dataset is specifically curated for binary sentiment classification, making it ideal for our purposes.\n",
    "\n",
    "2. **Explore the Dataset**\n",
    "* Once downloaded, unzip the file and explore its structure. You'll notice it's divided into two main directories: train and test.\n",
    "Each of these contains neg (negative) and pos (positive) subdirectories.\n",
    "\n",
    "* Familiarize yourself with the data by opening and reading a few sample files from both the neg and pos categories in both the train and test directories.\n",
    "This will give you a sense of the text data we'll be classifying.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "957t3IOp8t6L"
   },
   "source": [
    "Load train and test sets of the dataset and store them in lists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "py93WCku8sjf"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_QIsmD049wR"
   },
   "source": [
    "## Hand-coded Rules\n",
    "\n",
    "This section introduces you to the concept of rule-based classification, a fundamental approach in NLP. You'll create a simple sentiment analyzer based on keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCV_W-nyMbqT"
   },
   "source": [
    "1. **Prepare Keyword Sets**\n",
    "* Compile two lists of keywords: one representing positive sentiment ($PS$) and the other representing negative sentiment ($NS$).\n",
    "* Think about common words that clearly convey positivity or negativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FTH0SNxZ5CK1"
   },
   "outputs": [],
   "source": [
    "# Define lists of positive and negative keywords\n",
    "# PS =\n",
    "# NS ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFTU16R-MhW2"
   },
   "source": [
    "\n",
    "2. **Count Keyword Matches**\n",
    "* For a given input text $t$, calculate the number of words that appear in the $PS$ set ($n_{PS}^{t}$) and in the $NS$ set ($n_{NS}^{t}$).\n",
    "* This step involves processing the text to identify and count the matching words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NhPsCzuU77AS"
   },
   "outputs": [],
   "source": [
    "def count_keywords(text, PS, NS):\n",
    "  \"\"\"\n",
    "  Counts the number of positive and negative keywords in a given text.\n",
    "\n",
    "  Args:\n",
    "    text: The text to analyze.\n",
    "    PS: A set of positive keywords.\n",
    "    NS: A set of negative keywords.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the number of positive and negative keywords found.\n",
    "  \"\"\"\n",
    "  pass\n",
    "  #\n",
    "  # implement here\n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JlKG3jqMjL3"
   },
   "source": [
    "\n",
    "3. **Create Decision Rules**\n",
    "* Develop your own set of rules to classify the sentiment as Positive or Negative based on $n_{PS}^{t}$ and $n_{NS}^{t}$.\n",
    "* For example, you might decide that a text is positive if $n_{PS}^{t}$ is greater than $n_{NS}^{t}$, negative if the opposite is true, or neutral/undecided if the counts are equal.\n",
    "* Feel free to experiment with different rule combinations to see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LMF15Io8MjrB"
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(text, PS, NS):\n",
    "  \"\"\"\n",
    "  Classifies the sentiment of a given text based on keyword counts.\n",
    "\n",
    "  Args:\n",
    "    text: The text to analyze.\n",
    "    PS: A set of positive keywords.\n",
    "    NS: A set of negative keywords.\n",
    "\n",
    "  Returns:\n",
    "    1 for \"pos\" or 0 for \"neg\" depending on the keyword counts.\n",
    "  \"\"\"\n",
    "  pass\n",
    "  #\n",
    "  # implement here\n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GWX23se07xD"
   },
   "source": [
    "4. **Evaluation**\n",
    "\n",
    "* Apply your method to the test set and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TXRcDDpx1LRc"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29VE6ho2-nAK"
   },
   "source": [
    "## Naive Bayes Classifier with Bag of Words\n",
    "\n",
    "In this section, we'll step into the realm of machine learning by implementing a Naive Bayes Classifier, a popular and straightforward probabilistic classifier that is often effective in text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4KM0wn3-t2z"
   },
   "source": [
    "### Understand the Naive Bayes Principle\n",
    "* A probabilistic classifier that assumes independence between features.\n",
    "* In the context of text classification, it calculates the probability of a document belonging to a certain class (positive or negative) based on the probabilities of the words it contains.\n",
    "\n",
    "Assuming we have a document $d$ and we want to classify it into one of the classes $C$ (which in our case are 'positive' or 'negative'), the Naive Bayes classifier calculates the probability of $d$ belonging to a class $c$ in $C$ based on the words it contains.\n",
    "\n",
    "The equation for this can be written as:\n",
    "\n",
    "$$\n",
    "P(c|d) = \\frac{P(d|c) \\times P(c)}{P(d)}\n",
    "$$\n",
    "\n",
    "Where\n",
    "* $ P(c|d) $ is the posterior probability of class $ c $ given document $ d $.\n",
    "* $ P(d|c) $ is the likelihood of document $ d $ given class $ c $.\n",
    "* $ P(c) $ is the prior probability of class $ c $.\n",
    "* $ P(d) $ is the probability of document $ d $.\n",
    "\n",
    "For text classification:\n",
    "$$\n",
    "P(d|c) = P(w_1|c) \\times P(w_2|c) \\times ... \\times P(w_n|c)\n",
    "$$\n",
    "\n",
    "where $ d $ is composed of words $ w_1, w_2, ..., w_n $, and each $ P(w_i|c) $ is the probability of word $ w_i $ occurring in documents of class $ c $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie7K1utRAz10"
   },
   "source": [
    "### Feature Extraction: Bag of Words\n",
    "\n",
    "In this stage, you'll transform your text data into a format suitable for the Naive Bayes Classifier, using the 'Bag of Words' (BoW) model.\n",
    "\n",
    "Follow these steps to implement the BoW model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRSudSk3BCjv"
   },
   "source": [
    "**1**. **Text Preprocessing**\n",
    "* Remove punctuation and special characters.\n",
    "* Convert all text to lowercase.\n",
    "* Split text into individual words.\n",
    "* Remove commonly used words that do not contribute to the meaning of the text.\n",
    "  * e.g., use [NLTK](https://www.nltk.org/)'s stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x4cg9iub3Rtt"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIFLi9eBBpKF"
   },
   "source": [
    "2. **Vocabulary Building and Vectorization**\n",
    "* List all unique words from the dataset to form a vocabulary.\n",
    "* Represent each document as a vector, where each dimension corresponds to a word in the vocabulary, and the value is the frequency of the word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M01IcssFC27d"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01JMiZn-Bq3t"
   },
   "source": [
    "### Training a Naive Bayes Classifier\n",
    "\n",
    "**Note**: *Use the training set to train your model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmPwOff-CgUS"
   },
   "source": [
    "1. **Probability Calculation**:\n",
    "   - Calculate the prior probabilities for each class (positive and negative).\n",
    "   - For each class, count the frequency of each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "r4y8O-nKCfmg"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XueKDmeFCxSB"
   },
   "source": [
    "\n",
    "2. **Conditional Probability Calculation with Smoothing**:\n",
    "   - Compute the conditional probabilities $ P(w_i|c) $ for each word $ w_i $ in your vocabulary and each class $ c $, applying Laplace smoothing to handle zero probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2orn3l1KD6-p"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqg_79LlCzaV"
   },
   "source": [
    "3. **Classifier Implementation and Document Classification**:\n",
    "   - Implement the Naive Bayes formula to calculate $ P(c|d) $ for each document in the test set.\n",
    "   - Classify each document into the class with the highest calculated probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DmG0O4U6C0Sm"
   },
   "outputs": [],
   "source": [
    "def classify_document(document):\n",
    "  \"\"\"\n",
    "  Classifies a given document using the Naive Bayes classifier.\n",
    "\n",
    "  Args:\n",
    "    document: The text document to classify.\n",
    "\n",
    "  Returns:\n",
    "    The predicted class ('pos' or 'neg').\n",
    "  \"\"\"\n",
    "  pass\n",
    "  #\n",
    "  # implement here\n",
    "  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5zs1eY4MLSZ"
   },
   "source": [
    "4. **Performance Evaluation**\n",
    "   - Evaluate the classifier using metrics like accuracy, precision, recall, and F1 score, comparing the predicted class labels against the true labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KkN-MGnjMN02"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI26dWdeMNsz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N54biSiXMNf0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2wLCqReFM5P"
   },
   "source": [
    "## Naive Bayes Classifier with TF-IDF\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)** is a numerical statistic that reflects how important a word is to a document in a collection or corpus.\n",
    "\n",
    "It is often used in text mining and information retrieval to transform the textual data into a format more suitable for machine learning algorithms.\n",
    "\n",
    "The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "This helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "For a given document $ d $ and a class $ c $ in a set of classes $ C $, TF-IDF can be particularly insightful. It allows us to:\n",
    "\n",
    "1. **Distinguish Important Words in Each Document**: By calculating the TF-IDF score for each word in document $ d $, we can identify which words are particularly characteristic of that document compared to other documents in the corpus.\n",
    "\n",
    "2. **Understand Word Significance Across Classes**: In a classification task, TF-IDF can highlight which words are most descriptive of documents belonging to class $ c $, as opposed to those in other classes in $ C $.\n",
    "\n",
    "3. **Improve Classification Models**: When used as features in machine learning models, TF-IDF vectors provide a more nuanced representation of text data than simple word counts, potentially leading to more accurate predictions in tasks like sentiment analysis or topic classification.\n",
    "\n",
    "In the following section, we'll use Python and a machine learning library to apply TF-IDF to a text classification problem, leveraging its potential to enhance the performance of our Naive Bayes Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXg__lPsKIqQ"
   },
   "source": [
    "TF-IDF is calculated using two components: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "**Term Frequency (TF)**:\n",
    "TF measures how frequently a term occurs in a document.\n",
    "It's calculated as:\n",
    "\n",
    "$$\n",
    "     \\text{TF}(t, d) = \\frac{n_{t, d}}{n_{d}}\n",
    "$$\n",
    "where $ n_{t, d} $ is the number of occurrences of term $ t $ in document $ d $, and $ n_{d} $ is the total number of terms in document $d$.\n",
    "\n",
    "\n",
    "**Inverse Document Frequency (IDF)**:\n",
    "IDF measures how important a term is across a set of documents or corpus.\n",
    "It's calculated as:\n",
    "\n",
    "$$\n",
    "     \\text{IDF}(t, D) = \\log \\left( \\frac{N_D}{n_{D, t}} \\right)\n",
    "$$\n",
    "where $ N_D $ is the total number of documents in the corpus $ D $, and $ n_{D, t} $ is the number of documents with term $ t $.\n",
    "\n",
    "Finally, the TF-IDF score of a term is the product of its TF and IDF scores:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ t $ is a term.\n",
    "- $ d $ is a document.\n",
    "- $ D $ is the corpus of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N15C9eVkLxuM"
   },
   "source": [
    "1. **TF-IDF Vectorization**\n",
    "* Utilize scikit-learn's TfidfVectorizer to transform the text data into TF-IDF features.\n",
    "* This involves converting the raw documents into a matrix of TF-IDF features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5OhNJV1JcKEg"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVYFwKZYL2Zc"
   },
   "source": [
    "2. **Train the Naive Bayes Classifier**\n",
    "* Use scikit-learn's Naive Bayes implementation (like `MultinomialNB`) to train the model on the TF-IDF vectors.\n",
    "* Fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RcB1OQVzL7E8"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7P0P30KL7vE"
   },
   "source": [
    "3. **Model Evaluation**\n",
    "* Evaluate the performance of your model on the testing set.\n",
    "* Use metrics such as accuracy, precision, recall, and F1 score to understand the effectiveness of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BEGbAUEeckAS"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ejDoNc3MCFG"
   },
   "source": [
    "4. **Parameter Tuning (Optional)**\n",
    "* Experiment with different parameters of TfidfVectorizer and the Naive Bayes model to optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OTUFGiuWMDTH"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# implement here\n",
    "#\n",
    "\n",
    "# For examples:\n",
    "\n",
    "# 1. **Parameter Tuning for TfidfVectorizer**\n",
    "# - `max_features`: Control the maximum number of features in the vocabulary.\n",
    "# - `min_df`: Set the minimum document frequency below which a word is ignored.\n",
    "# - `max_df`: Set the maximum document frequency above which a word is ignored.\n",
    "# - `stop_words`: Specify a list of stop words to be excluded from the vocabulary.\n",
    "# - `ngram_range`: Specify the range of n-grams to be considered as features.\n",
    "#\n",
    "# 2. **Parameter Tuning for Naive Bayes Classifier**\n",
    "# - `alpha`: Adjust the smoothing parameter to control the influence of prior probabilities.\n",
    "# - `fit_prior`: Enable or disable learning class prior probabilities from the data.\n",
    "# - `class_prior`: Set prior probabilities for each class manually.\n",
    "#\n",
    "# 3. **Grid Search and Cross-Validation**\n",
    "# - Use a technique like grid search or randomized search to evaluate multiple combinations of parameters systematically.\n",
    "# - Perform cross-validation to assess the performance of different parameter settings objectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye3GOA0WMmNp"
   },
   "source": [
    "## Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwaJlEO_MrK_"
   },
   "source": [
    "1. Compare performance between classifers built in previous sections:\n",
    "\n",
    "* Hand-coded Rule\n",
    "* BoW\n",
    "* TF-IDF\n",
    "\n",
    "2. Are the results statistically significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIJrA_WSMqjK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}