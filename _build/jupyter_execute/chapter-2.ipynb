{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xjsOchF-6ig"
   },
   "source": [
    "# Understanding Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3h9cygG_vRC"
   },
   "source": [
    "Word embeddings are a revolutionary approach in natural language processing (NLP) that maps words or phrases from the vocabulary to vectors of real numbers.\n",
    "\n",
    "This method represents words in a dense vector space where semantically similar words are mapped to nearby points. Word embeddings capture the essence of word meanings, relationships, and the context in which they appear, enabling machines to understand and process language much like humans do.\n",
    "\n",
    "### Key Concepts of Word Embeddings\n",
    "\n",
    "- **Semantic Similarity**: Words that are used in similar contexts are embedded closely together in the vector space. For example, \"king\" and \"queen\" are closer than \"king\" and \"apple\".\n",
    "- **Dimensionality Reduction**: Embeddings reduce the dimensionality of the text data, converting sparse, high-dimensional vectors (like one-hot encoded vectors) into dense, lower-dimensional forms. This improves model efficiency and performance.\n",
    "\n",
    "### Representative Learning Algorithms\n",
    "\n",
    "Several algorithms are pivotal in generating word embeddings. Here are a few notable ones:\n",
    "\n",
    "1. **Word2Vec**: Introduced by Mikolov et al., Word2Vec offers two architecture choices for learning embeddings: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
    "\n",
    "  - **CBOW** predicts a target word based on context words. The objective function it optimizes is:\n",
    "\n",
    "  $$\n",
    "      \\max_{\\theta} \\sum_{w \\in C} \\log p(w|C; \\theta)\n",
    "  $$\n",
    "  where \\(C\\) represents the context words, \\(w\\) is the target word, and \\(\\theta\\) are the parameters of the model.\n",
    "\n",
    "  - **Skip-Gram** works in the opposite manner, predicting context words from a target word. Its objective function is:\n",
    "\n",
    "   $$\n",
    "    \\max_{\\theta} \\sum_{w \\in C} \\log p(C|w; \\theta)\n",
    "   $$\n",
    "\n",
    "2. **GloVe (Global Vectors for Word Representation)**: GloVe is another widely-used method that focuses on word co-occurrences in the corpus. The model learns by minimizing the difference between the dot product of the embeddings of two words and the logarithm of their co-occurrence probability:\n",
    "  $$\n",
    "      J(\\theta) = \\sum_{i,j=1}^{V} f(X_{ij}) (w_i^T w_j + b_i + b_j - \\log X_{ij})^2\n",
    "  $$\n",
    "\n",
    "  where $w_i$ and $w_j$ are the word embeddings for words $i$ and $j$, $b_i$ and $b_j$ are the biases for words $i$ and $j$, $X_{ij}$ is the number of times word $i$ appears in the context of word $j$, and $f$ is a weighting function applied to $X_{ij}$.\n",
    "\n",
    "### Applications of Word Embeddings\n",
    "\n",
    "Word embeddings are foundational for various NLP tasks, including text classification, sentiment analysis, machine translation, and more. By understanding and applying these embeddings, we can significantly enhance the performance of NLP models, making them more nuanced and effective in handling language.\n",
    "\n",
    "In the next sections, we'll explore how to leverage these embeddings for text representation and delve into advanced neural network models for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Kpgv5fbCWWf"
   },
   "source": [
    "### Exercise 1: Exploring Word Embeddings through Analogy Tasks\n",
    "\n",
    "* Use a pre-trained Word2Vec or GloVe model available in libraries like `gensim` or `spaCy`.\n",
    "  * e.g. `word2vec-google-news-300`\n",
    "\n",
    "* Solve the following analogy tasks using the model:\n",
    "\n",
    "  ```\n",
    "  king - man + woman = ?\n",
    "  paris - france + italy = ?\n",
    "  einstein - scientist + artist = ?\n",
    "  ```\n",
    "\n",
    "* Evaluate performance of the model on [Google dataset](http://download.tensorflow.org/data/questions-words.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mj6VWVUER-TF",
    "outputId": "0d4dcab3-d1f3-4475-dd04-5d1058ce606d"
   },
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "# model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# implement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko6JfHIKEg1-"
   },
   "source": [
    "### Exercise 2: Visualizing Word Embeddings\n",
    "\n",
    "* Choose a diverse set of words\n",
    "  * For example, words related to professions, animals, and emotions.\n",
    "  * [Occupation list](https://gist.github.com/wsc/1083459)\n",
    "  * [Emotion list](https://raw.githubusercontent.com/imsky/wordlists/master/adjectives/emotions.txt)\n",
    "\n",
    "* Use the same pre-trained model to extract embeddings for the selected words.\n",
    "\n",
    "* Use the available library (e.g. `sklearn.manifold`) to reduce the dimensionality of the embeddings to 2 dimensions. Fit the model to the embeddings and transform them into a 2D space.\n",
    "\n",
    "* Plot the 2D embeddings. Label each point with its corresponding word to see how words cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1NY8YwebF8Ut"
   },
   "outputs": [],
   "source": [
    "# implement here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}